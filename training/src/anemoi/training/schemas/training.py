# (C) Copyright 2024-2025 ECMWF.
#
# This software is licensed under the terms of the Apache Licence Version 2.0
# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
# In applying this licence, ECMWF does not waive the privileges and immunities
# granted to it by virtue of its status as an intergovernmental organisation
# nor does it submit to any jurisdiction.


from __future__ import annotations

from enum import Enum
from functools import partial
from typing import Annotated
from typing import Any
from typing import Literal
from typing import Union

from pydantic import AfterValidator
from pydantic import Field
from pydantic import NonNegativeFloat
from pydantic import NonNegativeInt
from pydantic import PositiveInt
from pydantic import field_validator
from pydantic import model_validator

from anemoi.training.schemas.utils import allowed_values

from .utils import BaseModel


class GradientClip(BaseModel):
    """Gradient clipping configuration."""

    val: float = 32.0
    "Gradient clipping value."
    algorithm: Annotated[str, AfterValidator(partial(allowed_values, values=["value", "norm"]))] = Field(
        example="value",
    )
    "The gradient clipping algorithm to use"


class SWA(BaseModel):
    """Stochastic weight averaging configuration.

    See https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/
    """

    enabled: bool = Field(example=False)
    "Enable stochastic weight averaging."
    lr: NonNegativeFloat = Field(example=1.0e-4)
    "Learning rate for SWA."


class Rollout(BaseModel):
    """Rollout configuration."""

    start: PositiveInt = Field(example=1)
    "Number of rollouts to start with."
    epoch_increment: NonNegativeInt = Field(example=0)
    "Number of epochs to increment the rollout."
    max: PositiveInt = Field(example=1)
    "Maximum number of rollouts."


class LR(BaseModel):
    """Learning rate configuration.

    Changes in per-gpu batch_size should come with a rescaling of the local_lr,
    in order to keep a constant global_lr global_lr = local_lr * num_gpus_per_node * num_nodes / gpus_per_model.
    """

    rate: NonNegativeFloat = Field(example=0.625e-4)  # TODO(Helen): Could be computed by pydantic
    "Initial learning rate. Is adjusteed according to the hardware configuration"
    iterations: NonNegativeInt = Field(example=300000)
    "Number of iterations."
    min: NonNegativeFloat = Field(example=3e-7)
    "Minimum learning rate."
    warmup_t: NonNegativeInt = Field(example=1000)
    "Number of warm up iteration. Default to 1000."


class LossScalingSchema(BaseModel):
    default: int = 1
    "Default scaling value applied to the variables loss. Default to 1."
    pl: dict[str, NonNegativeFloat]
    "Scaling value associated to each pressure level variable loss."
    sfc: dict[str, NonNegativeFloat]
    "Scaling value associated to each surface variable loss."


class GeneralVariableLossScalerSchema(BaseModel):
    target_: Literal["anemoi.training.losses.scalers.GeneralVariableLossScaler"] = Field(..., alias="_target_")
    weights: dict[str, float]
    "Weight of each variable."  # Check keys (variables) are read ???


class NaNMaskScalerSchema(BaseModel):
    target_: Literal["anemoi.training.losses.scalers.NaNMaskScalerSchema"] = Field(..., alias="_target_")


class TendencyScalerTargets(str, Enum):
    stdev = "anemoi.training.losses.scalers.StdevTendencyScaler"
    var = "anemoi.training.losses.scalers.VarTendencyScaler"


class TendencyScalerSchema(BaseModel):
    target_: TendencyScalerTargets = Field(
        example="anemoi.training.losses.scalers.StdevTendencyScaler",
        alias="_target_",
    )


class PressureLevelScalerTargets(str, Enum):
    relu_scaler = "anemoi.training.losses.scalers.ReluPressureLevelScaler"
    linear_scaler = "anemoi.training.losses.scalers.LinearPressureLevelScaler"
    polynomial_sclaer = "anemoi.training.losses.scalers.PolynomialPressureLevelScaler"
    no_scaler = "anemoi.training.losses.scalers.NoPressureLevelScaler"


class PressureLevelScalerSchema(BaseModel):
    target_: PressureLevelScalerTargets = Field(
        example="anemoi.training.losses.scalers.ReluPressureLevelScaler",
        alias="_target_",
    )
    group: str = Field(example="pl")
    "Group of variables to scale."
    minimum: float = Field(example=0.2)
    "Minimum value of the scaling function."
    y_intercept: float = 0.001
    "Y-axis shift of scaling function."


class GraphNodeAttributeScalerSchema(BaseModel):
    target_: Literal["anemoi.training.losses.scalers.GraphNodeAttributeScaler"] = Field(..., alias="_target_")
    nodes_name: str = Field(example="data")
    "Name of the nodes to take the attribute from."
    nodes_attribute_name: str = Field(example="area_weight")
    "Name of the node attribute to return."
    apply_output_mask: bool = Field(example=False)
    "Whether to apply the output mask to the node attribute values."
    norm: Literal["unit-max", "unit-sum"] = Field(example="unit-sum")
    "Normalisation method applied to the node attribute."


class ReweightedGraphNodeAttributeScalerSchema(BaseModel):
    target_: Literal["anemoi.training.losses.scalers.ReweightedGraphNodeAttributeScalerSchema"] = Field(
        ...,
        alias="_target_",
    )
    nodes_name: str = Field(example="data")
    "Name of the nodes to take the attribute from."
    nodes_attribute_name: str = Field(example="area_weight")
    "Name of the node attribute to return."
    scaling_mask_attribute_name: str = Field(example="cutout_mask")
    "Name of the node attribute to use as a mask to reweight the reference values."
    weight_frac_of_total: float = Field(example=0.5)
    "Fraction of total weight to assign to nodes within the scaling mask. The remaining weight is distributed among "
    "nodes outside the mask."
    apply_output_mask: bool = Field(example=False)
    "Whether to apply the output mask to the node attribute values."
    norm: Literal["unit-max", "unit-sum"] = Field(example="unit-sum")
    "Normalisation method applied to the node attribute."


ScalerSchema = Union[
    GeneralVariableLossScalerSchema,
    PressureLevelScalerSchema,
    TendencyScalerSchema,
    NaNMaskScalerSchema,
    GraphNodeAttributeScalerSchema,
    ReweightedGraphNodeAttributeScalerSchema,
]


class ImplementedLossesUsingBaseLossSchema(str, Enum):
    rmse = "anemoi.training.losses.RMSELoss"
    mse = "anemoi.training.losses.MSELoss"
    mae = "anemoi.training.losses.MAELoss"
    logcosh = "anemoi.training.losses.LogCoshLoss"
    huber = "anemoi.training.losses.HuberLoss"


class BaseLossSchema(BaseModel):
    target_: ImplementedLossesUsingBaseLossSchema = Field(..., alias="_target_")
    "Loss function object from anemoi.training.losses."
    scalers: list[str] = Field(example=["variable"])  # TODO(Mario): Validate scalers are defined
    "Scalars to include in loss calculation"
    ignore_nans: bool = False
    "Allow nans in the loss and apply methods ignoring nans for measuring the loss."


class HuberLossSchema(BaseLossSchema):
    delta: float = 1.0
    "Threshold for Huber loss."


class CombinedLossSchema(BaseLossSchema):
    target_: Literal["anemoi.training.losses.combined.CombinedLoss"] = Field(..., alias="_target_")
    losses: list[BaseLossSchema] = Field(min_length=1)
    "Losses to combine, can be any of the normal losses."
    loss_weights: Union[list[Union[int, float]], None] = None
    "Weightings of losses, if not set, all losses are weighted equally."

    @field_validator("losses", mode="before")
    @classmethod
    def add_empty_scalars(cls, losses: Any) -> Any:
        """Add empty scalars to loss functions, as scalars can be set at top level."""
        from omegaconf.omegaconf import open_dict

        for loss in losses:
            if "scalars" not in loss:
                with open_dict(loss):
                    loss["scalars"] = []
        return losses

    @model_validator(mode="after")
    def check_length_of_weights_and_losses(self) -> CombinedLossSchema:
        """Check that the number of losses and weights match, or if not set, skip."""
        losses, loss_weights = self.losses, self.loss_weights
        if loss_weights is not None and len(losses) != len(loss_weights):
            error_msg = "Number of losses and weights must match"
            raise ValueError(error_msg)
        return self


LossSchemas = Union[BaseLossSchema, HuberLossSchema, CombinedLossSchema]


class ScaleValidationMetricsSchema(BaseModel):
    """Configuration for scaling validation metrics.

    Here variable scaling is possible due to the metrics being calculated in the same way as the
    training loss, within internal model space.
    """

    scalers_to_apply: list[str] = Field(example=["variable"])
    """List of scalars to be applied."""
    metrics: list[str]
    """List of metrics to keep in normalised space.."""


class TrainingSchema(BaseModel):
    """Training configuration."""

    run_id: Union[str, None] = Field(example=None)
    "Run ID: used to resume a run from a checkpoint, either last.ckpt or specified in hardware.files.warm_start."
    fork_run_id: Union[str, None] = Field(example=None)
    "Run ID to fork from, either last.ckpt or specified in hardware.files.warm_start."
    load_weights_only: bool = Field(example=False)
    "Load only the weights from the checkpoint, not the optimiser state."
    transfer_learning: bool = Field(example=False)
    "Flag to activate transfer learning mode when loading a checkpoint."
    submodules_to_freeze: list[str] = Field(example=["processor"])
    "List of submodules to freeze during transfer learning."
    deterministic: bool = Field(default=False)
    "This flag sets the torch.backends.cudnn.deterministic flag. Might be slower, but ensures reproducibility."
    precision: str = Field(default="16-mixed")
    "Precision"
    multistep_input: PositiveInt = Field(example=2)
    """Number of input steps for the model. E.g. 1 = single step scheme, X(t-1) used to predict X(t),
    k > 1: multistep scheme, uses [X(t-k), X(t-k+1), ... X(t-1)] to predict X(t)."""
    accum_grad_batches: PositiveInt = Field(default=1)
    """Accumulates gradients over k batches before stepping the optimizer.
    K >= 1 (if K == 1 then no accumulation). The effective bacthsize becomes num-device * k."""
    num_sanity_val_steps: PositiveInt = Field(example=6)
    "Sanity check runs n batches of val before starting the training routine."
    gradient_clip: GradientClip
    "Config for gradient clipping."
    swa: SWA = Field(default_factory=SWA)
    "Config for stochastic weight averaging."
    zero_optimizer: bool = Field(example=False)
    "use ZeroRedundancyOptimizer, saves memory for larger models."
    training_loss: LossSchemas
    "Training loss configuration."
    loss_gradient_scaling: bool = False
    "Dynamic rescaling of the loss gradient. Not yet tested."
    scalers: dict[str, ScalerSchema]
    "Scalers to use in the computation of the loss and validation scores."
    validation_metrics: dict[str, LossSchemas]
    "List of validation metrics configurations. These metrics "
    scale_validation_metrics: ScaleValidationMetricsSchema
    """Configuration for scaling validation metrics."""
    rollout: Rollout = Field(default_factory=Rollout)
    "Rollout configuration."
    max_epochs: Union[PositiveInt, None] = None
    "Maximum number of epochs, stops earlier if max_steps is reached first."
    max_steps: PositiveInt = 150000
    "Maximum number of steps, stops earlier if max_epochs is reached first."
    lr: LR = Field(default_factory=LR)
    "Learning rate configuration."
    metrics: list[str]
    "List of metrics"
